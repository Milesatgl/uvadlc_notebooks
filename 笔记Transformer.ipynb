{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在自注意力机制（Self-Attention）中的 Scaled Dot Product Attention 运算中，\\( d_k \\) 和 \\( d_v \\) 分别代表用于查询/键（queries/keys）和用于值（values）的隐藏维度。\n",
    "\n",
    "### Hidden Dimensionality 的理解：\n",
    "\n",
    "1. **查询（Query）、键（Key）和值（Value）向量的表示**：\n",
    "   - 在自注意力中，每个输入会被投影（通过线性变换）生成查询、键和值。在这个过程中，原始输入向量被映射到新的向量空间，这个新的向量空间的大小就是 \"hidden dimensionality\"。\n",
    "   \n",
    "2. **\\( d_k \\)（queries/keys 的维度）**：\n",
    "   - 表示查询和键向量的维度。这意味着每个查询和键在隐藏空间中被表示为一个 \\( d_k \\) 维的向量。\n",
    "   - 这个维度直接影响了注意力得分的计算，因为注意力得分是通过查询和键之间的点积计算得到的。\n",
    "\n",
    "3. **\\( d_v \\)（values 的维度）**：\n",
    "   - 表示值向量的维度。即每个值在隐藏空间中被表示为一个 \\( d_v \\) 维的向量。\n",
    "   - 值的维度决定了由注意力得分加权生成的最终表示的维度。因此，输出维度通常是 \\( d_v \\)。\n",
    "\n",
    "### 为什么使用 Hidden Dimensionality：\n",
    "\n",
    "- **灵活性和表达能力**：通过选择合适的 \\( d_k \\) 和 \\( d_v \\)，可以灵活地调整模型的表达能力和计算复杂性。\n",
    "- **计算效率**：通常 \\( d_k \\) 会被设置得相对较小以提升计算的效率，尤其是在需要对很长序列进行计算时。\n",
    "- **模型性能**：在实践中，选择合适的隐层维度对于模型的性能和泛化能力至关重要。\n",
    "\n",
    "在实现中，通过线性层（全连接层）对输入进行变换来获得所需的 \\( d_k \\) 和 \\( d_v \\) 维度，这些维度可以单独选择来适配特定的应用需求或资源限制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
